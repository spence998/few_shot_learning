{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9744bcdc-60c3-48f0-9137-1a34ced84ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import importlib.util\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import IterableDataset\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2bd784-191e-4593-b2da-551a0bd29b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    \"\"\"\n",
    "    if nb_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, nb_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    image_labels = [\n",
    "        (i, os.path.join(path, image))\n",
    "        for i, path in zip(labels, paths)\n",
    "        for image in sampler(os.listdir(path))\n",
    "    ]\n",
    "    if shuffle:\n",
    "        random.shuffle(image_labels)\n",
    "    return image_labels\n",
    "\n",
    "\n",
    "class DataGenerator(IterableDataset):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_samples_per_class,\n",
    "        batch_type,\n",
    "        config={},\n",
    "        cache=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of classes for classification (N-way)\n",
    "            num_samples_per_class: num samples to generate per class in one batch (K+1)\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "            batch_type: train/val/test\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        data_folder = config.get(\"data_folder\", \"./omniglot_resized\")\n",
    "        self.img_size = config.get(\"img_size\", (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [\n",
    "            os.path.join(data_folder, family, character)\n",
    "            for family in os.listdir(data_folder)\n",
    "            if os.path.isdir(os.path.join(data_folder, family))\n",
    "            for character in os.listdir(os.path.join(data_folder, family))\n",
    "            if os.path.isdir(os.path.join(data_folder, family, character))\n",
    "        ]\n",
    "\n",
    "        random.seed(1)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[:num_train]\n",
    "        self.metaval_character_folders = character_folders[num_train : num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[num_train + num_val :]\n",
    "        self.image_caching = cache\n",
    "        self.stored_images = {}\n",
    "\n",
    "        if batch_type == \"train\":\n",
    "            self.folders = self.metatrain_character_folders\n",
    "        elif batch_type == \"val\":\n",
    "            self.folders = self.metaval_character_folders\n",
    "        else:\n",
    "            self.folders = self.metatest_character_folders\n",
    "\n",
    "    def image_file_to_array(self, filename, dim_input):\n",
    "        \"\"\"Takes an image path and returns numpy array\"\"\"\n",
    "        if self.image_caching and (filename in self.stored_images):\n",
    "            return self.stored_images[filename]\n",
    "        image = imageio.imread(filename)  # misc.imread(filename)\n",
    "        image = image.reshape([dim_input])\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = 1.0 - image\n",
    "        if self.image_caching:\n",
    "            self.stored_images[filename] = image\n",
    "        return image\n",
    "\n",
    "    def _sample(self):\n",
    "        \"\"\"Samples a batch for training, validation, or testing\"\"\"\n",
    "        K, N = self.num_samples_per_class, self.num_classes\n",
    "\n",
    "        sampled_classes = random.sample(self.folders, N)\n",
    "        one_hot_labels = np.eye(N)\n",
    "\n",
    "        images_and_labels = get_images(sampled_classes, one_hot_labels, nb_samples=K, shuffle=False)\n",
    "\n",
    "        support_labels, support_images = [], []\n",
    "        query_labels, query_images = [], []\n",
    "        for i, (label, image) in enumerate(images_and_labels):\n",
    "            if i % K == 0:\n",
    "                support_labels.append(label)\n",
    "                support_images.append(image)\n",
    "            else:\n",
    "                query_labels.append(label)\n",
    "                query_images.append(image)\n",
    "\n",
    "        zip_test_label_images = list(zip(query_labels, query_images))\n",
    "        np.random.shuffle(zip_test_label_images)\n",
    "        query_labels, query_images = zip(*zip_test_label_images)\n",
    "\n",
    "        images_support = [self.image_file_to_array(x, self.dim_input) for x in support_images]\n",
    "        images_query = [self.image_file_to_array(x, self.dim_input) for x in query_images]\n",
    "\n",
    "        images = np.array(images_support+images_query).reshape(K, N, self.dim_input)\n",
    "        labels = np.array(support_labels + [q for q in query_labels]).reshape(K, N, N)\n",
    "\n",
    "        images = images.astype(\"float32\")\n",
    "        labels = labels.astype(\"float32\")\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield self._sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bef960-0e15-439a-bfb9-65e108d2de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    if type(model) in [nn.Linear]:\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        nn.init.zeros_(model.bias)\n",
    "    elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "        nn.init.orthogonal_(model.weight_hh_l0)\n",
    "        nn.init.xavier_uniform_(model.weight_ih_l0)\n",
    "        nn.init.zeros_(model.bias_hh_l0)\n",
    "        nn.init.zeros_(model.bias_ih_l0)\n",
    "\n",
    "\n",
    "class MANN(nn.Module):\n",
    "    def __init__(self, num_classes, samples_per_class, hidden_dim):\n",
    "        super(MANN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "\n",
    "        self.layer1 = torch.nn.LSTM(num_classes + 784, hidden_dim, batch_first=True)\n",
    "        self.layer2 = torch.nn.LSTM(hidden_dim, num_classes, batch_first=True)\n",
    "        initialize_weights(self.layer1)\n",
    "        initialize_weights(self.layer2)\n",
    "\n",
    "    def forward(self, input_images, input_labels):\n",
    "        B, Kplusone, N, D = input_images.shape\n",
    "        zero_labels = torch.zeros_like(input_labels[:, -1:, :, :])\n",
    "        labels = torch.concat([input_labels[:, :-1, :, :], zero_labels], dim=1)\n",
    "\n",
    "        x = torch.concat([input_images, labels], dim=3)\n",
    "        x = x.reshape(\n",
    "            x.shape[0],\n",
    "            x.shape[1] * x.shape[2],\n",
    "            x.shape[3],\n",
    "        )\n",
    "\n",
    "        x, _ = self.layer1(x)\n",
    "        out, _ = self.layer2(x)\n",
    "\n",
    "        out = out.reshape(B, Kplusone, N, N)\n",
    "        return out\n",
    "        \n",
    "    def loss_function(self, preds, labels):\n",
    "        \"\"\"Computes MANN loss\"\"\"\n",
    "        loss = None\n",
    "\n",
    "        query_preds, query_labels = preds[:, -1, :, :], labels[:, -1, :, :]\n",
    "\n",
    "        query_preds = query_preds.reshape(query_preds.shape[0]*self.num_classes, -1)\n",
    "        query_labels = query_labels.reshape(query_labels.shape[0]*self.num_classes, -1)\n",
    "\n",
    "        loss = F.cross_entropy(query_preds, query_labels)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000d49f0-8813-4a11-9bbd-d39535663415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels, model, optim, eval=False):\n",
    "    predictions = model(images, labels)\n",
    "    loss = model.loss_function(predictions, labels)\n",
    "    if not eval:\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return predictions.detach(), loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a5008e-7413-451e-a51d-f15ec410ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    print(config)\n",
    "    random.seed(config.random_seed)\n",
    "    np.random.seed(config.random_seed)\n",
    "    \n",
    "    if config.device == \"gpu\" and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        # Waiting for PyTorch 2.0 to enable MPS\n",
    "        # device = torch.device(\"mps\")\n",
    "        \n",
    "        # Default to cpu as for now mps it is not stable. \n",
    "        device = torch.device(\"cpu\")\n",
    "    elif config.device == \"gpu\" and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(\"Using device: \", device)\n",
    "\n",
    "    torch.manual_seed(config.random_seed)\n",
    "    \n",
    "    assert os.path.isdir(\"./omniglot_resized\")\n",
    "\n",
    "    # Create Data Generator\n",
    "    train_iterable = DataGenerator(\n",
    "        config.num_classes,\n",
    "        config.num_shot + 1,\n",
    "        batch_type=\"train\",\n",
    "        cache=config.image_caching,\n",
    "    )\n",
    "    train_loader = iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            train_iterable,\n",
    "            batch_size=config.meta_batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    )\n",
    "    test_iterable = DataGenerator(\n",
    "        config.num_classes,\n",
    "        config.num_shot + 1,\n",
    "        batch_type=\"test\",\n",
    "        cache=config.image_caching,\n",
    "    )\n",
    "    test_loader = iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            test_iterable,\n",
    "            batch_size=config.meta_batch_size,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = MANN(config.num_classes, config.num_shot + 1, config.hidden_dim)\n",
    "    model.to(device)\n",
    "\n",
    "    # Create optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    times = []\n",
    "    for step in range(config.train_steps):\n",
    "        ## Sample Batch\n",
    "        t0 = time.time()\n",
    "        i, l = next(train_loader)\n",
    "        i, l = i.to(device), l.to(device)\n",
    "        t1 = time.time()\n",
    "\n",
    "        ## Train\n",
    "        _, ls = train_step(i, l, model, optim)\n",
    "        t2 = time.time()\n",
    "        times.append([t1 - t0, t2 - t1])\n",
    "\n",
    "        ## Evaluate\n",
    "        if (step + 1) % config.eval_freq == 0:\n",
    "            if config.debug == True:\n",
    "                print(\"*\" * 5 + \"Iter \" + str(step + 1) + \"*\" * 5)\n",
    "            i, l = next(test_loader)\n",
    "            i, l = i.to(device), l.to(device)\n",
    "            pred, tls = train_step(i, l, model, optim, eval=True)\n",
    "            print(\"Train Loss:\", ls.cpu().numpy(), \"Test Loss:\", tls.cpu().numpy())\n",
    "            pred = torch.reshape(\n",
    "                pred, [-1, config.num_shot + 1, config.num_classes, config.num_classes]\n",
    "            )\n",
    "\n",
    "            pred = torch.argmax(pred[:, -1, :, :], axis=2)\n",
    "            l = torch.argmax(l[:, -1, :, :], axis=2)\n",
    "            acc = pred.eq(l).sum().item() / (config.meta_batch_size * config.num_classes)\n",
    "            print(\"Test Accuracy\", acc)\n",
    "\n",
    "            times = np.array(times)\n",
    "            print(f\"Sample time {times[:, 0].mean()} Train time {times[:, 1].mean()}\")\n",
    "            times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470b3b23-3df3-45d7-b5f7-c599837fce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(num_classes=2, num_shot=1, num_workers=4, eval_freq=100, meta_batch_size=128, hidden_dim=256, random_seed=123, learning_rate=0.001, train_steps=5000, image_caching=True, device='cpu', debug=True, cache=False)\n",
      "Using device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_5480\\2138218739.py:79: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)  # misc.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Iter 100*****\n",
      "Train Loss: 0.6764758 Test Loss: 0.68418396\n",
      "Test Accuracy 0.5234375\n",
      "Sample time 0.3911420273780823 Train time 0.10928846120834351\n",
      "*****Iter 200*****\n",
      "Train Loss: 0.59750336 Test Loss: 0.53428966\n",
      "Test Accuracy 0.72265625\n",
      "Sample time 0.15379758834838866 Train time 0.16037741422653198\n",
      "*****Iter 300*****\n",
      "Train Loss: 0.5734464 Test Loss: 0.5729797\n",
      "Test Accuracy 0.64453125\n",
      "Sample time 0.07374746322631837 Train time 0.10701999664306641\n",
      "*****Iter 400*****\n",
      "Train Loss: 0.53648233 Test Loss: 0.53488576\n",
      "Test Accuracy 0.703125\n",
      "Sample time 0.07112741947174073 Train time 0.11463311910629273\n",
      "*****Iter 500*****\n",
      "Train Loss: 0.522375 Test Loss: 0.52411914\n",
      "Test Accuracy 0.69921875\n",
      "Sample time 0.06677749156951904 Train time 0.1030724835395813\n",
      "*****Iter 600*****\n",
      "Train Loss: 0.50266016 Test Loss: 0.5433821\n",
      "Test Accuracy 0.703125\n",
      "Sample time 0.07270204067230225 Train time 0.10892043828964233\n",
      "*****Iter 700*****\n",
      "Train Loss: 0.5039217 Test Loss: 0.5193359\n",
      "Test Accuracy 0.75390625\n",
      "Sample time 0.05967942237854004 Train time 0.09189064264297485\n",
      "*****Iter 800*****\n",
      "Train Loss: 0.4841185 Test Loss: 0.46989393\n",
      "Test Accuracy 0.8203125\n",
      "Sample time 0.06009817838668823 Train time 0.08996725797653199\n",
      "*****Iter 900*****\n",
      "Train Loss: 0.4309653 Test Loss: 0.4359969\n",
      "Test Accuracy 0.859375\n",
      "Sample time 0.05662914276123047 Train time 0.08841093778610229\n",
      "*****Iter 1000*****\n",
      "Train Loss: 0.38923994 Test Loss: 0.42041942\n",
      "Test Accuracy 0.85546875\n",
      "Sample time 0.061328463554382324 Train time 0.09455173254013062\n",
      "*****Iter 1100*****\n",
      "Train Loss: 0.40082985 Test Loss: 0.4019331\n",
      "Test Accuracy 0.8828125\n",
      "Sample time 0.06281827926635743 Train time 0.09738170623779296\n",
      "*****Iter 1200*****\n",
      "Train Loss: 0.43778735 Test Loss: 0.43758208\n",
      "Test Accuracy 0.84375\n",
      "Sample time 0.06296840190887451 Train time 0.09722766876220704\n",
      "*****Iter 1300*****\n",
      "Train Loss: 0.40685034 Test Loss: 0.42199212\n",
      "Test Accuracy 0.87109375\n",
      "Sample time 0.054709174633026124 Train time 0.08478094577789307\n",
      "*****Iter 1400*****\n",
      "Train Loss: 0.4218703 Test Loss: 0.41339356\n",
      "Test Accuracy 0.86328125\n",
      "Sample time 0.06188772678375244 Train time 0.09641518354415894\n",
      "*****Iter 1500*****\n",
      "Train Loss: 0.3953989 Test Loss: 0.43060622\n",
      "Test Accuracy 0.86328125\n",
      "Sample time 0.05938832521438599 Train time 0.09064163684844971\n",
      "*****Iter 1600*****\n",
      "Train Loss: 0.4136021 Test Loss: 0.38872427\n",
      "Test Accuracy 0.90234375\n",
      "Sample time 0.07109996557235718 Train time 0.10829621315002441\n",
      "*****Iter 1700*****\n",
      "Train Loss: 0.40448248 Test Loss: 0.34835434\n",
      "Test Accuracy 0.93359375\n",
      "Sample time 0.0742181420326233 Train time 0.11113155364990235\n",
      "*****Iter 1800*****\n",
      "Train Loss: 0.3831678 Test Loss: 0.38203284\n",
      "Test Accuracy 0.90625\n",
      "Sample time 0.07579903125762939 Train time 0.11808933973312379\n",
      "*****Iter 1900*****\n",
      "Train Loss: 0.3200022 Test Loss: 0.36826074\n",
      "Test Accuracy 0.90625\n",
      "Sample time 0.08392802715301513 Train time 0.1360291051864624\n",
      "*****Iter 2000*****\n",
      "Train Loss: 0.3559754 Test Loss: 0.34533635\n",
      "Test Accuracy 0.921875\n",
      "Sample time 0.07594977617263794 Train time 0.11938122272491455\n",
      "*****Iter 2100*****\n",
      "Train Loss: 0.3843025 Test Loss: 0.38333142\n",
      "Test Accuracy 0.89453125\n",
      "Sample time 0.06957022666931152 Train time 0.1092788553237915\n",
      "*****Iter 2200*****\n",
      "Train Loss: 0.35929924 Test Loss: 0.39112312\n",
      "Test Accuracy 0.88671875\n",
      "Sample time 0.07174350261688232 Train time 0.11093171358108521\n",
      "*****Iter 2300*****\n",
      "Train Loss: 0.4006989 Test Loss: 0.39197716\n",
      "Test Accuracy 0.88671875\n",
      "Sample time 0.07050947666168213 Train time 0.11034052848815917\n",
      "*****Iter 2400*****\n",
      "Train Loss: 0.40722573 Test Loss: 0.42865843\n",
      "Test Accuracy 0.86328125\n",
      "Sample time 0.07999876976013183 Train time 0.12853847503662108\n",
      "*****Iter 2500*****\n",
      "Train Loss: 0.36639658 Test Loss: 0.3256729\n",
      "Test Accuracy 0.94921875\n",
      "Sample time 0.06831871032714844 Train time 0.10597135305404663\n",
      "*****Iter 2600*****\n",
      "Train Loss: 0.38011727 Test Loss: 0.4174627\n",
      "Test Accuracy 0.875\n",
      "Sample time 0.07525930404663086 Train time 0.1181441330909729\n",
      "*****Iter 2700*****\n",
      "Train Loss: 0.35501447 Test Loss: 0.3800843\n",
      "Test Accuracy 0.90234375\n",
      "Sample time 0.05703829050064087 Train time 0.08947170734405517\n",
      "*****Iter 2800*****\n",
      "Train Loss: 0.32777402 Test Loss: 0.35245842\n",
      "Test Accuracy 0.93359375\n",
      "Sample time 0.07161495923995971 Train time 0.10929828166961669\n",
      "*****Iter 2900*****\n",
      "Train Loss: 0.35616338 Test Loss: 0.37315917\n",
      "Test Accuracy 0.91015625\n",
      "Sample time 0.06703304052352906 Train time 0.10147907495498658\n",
      "*****Iter 3000*****\n",
      "Train Loss: 0.33429506 Test Loss: 0.3824069\n",
      "Test Accuracy 0.890625\n",
      "Sample time 0.06385790824890136 Train time 0.09953224658966064\n",
      "*****Iter 3100*****\n",
      "Train Loss: 0.36455372 Test Loss: 0.34439135\n",
      "Test Accuracy 0.91796875\n",
      "Sample time 0.06360827207565307 Train time 0.09819172143936157\n",
      "*****Iter 3200*****\n",
      "Train Loss: 0.34223735 Test Loss: 0.4149884\n",
      "Test Accuracy 0.859375\n",
      "Sample time 0.07022754669189453 Train time 0.10796791791915894\n",
      "*****Iter 3300*****\n",
      "Train Loss: 0.32842398 Test Loss: 0.37286854\n",
      "Test Accuracy 0.90234375\n",
      "Sample time 0.0719599986076355 Train time 0.10900065898895264\n",
      "*****Iter 3400*****\n",
      "Train Loss: 0.3710775 Test Loss: 0.35247087\n",
      "Test Accuracy 0.92578125\n",
      "Sample time 0.08158194303512573 Train time 0.12228133201599121\n",
      "*****Iter 3500*****\n",
      "Train Loss: 0.33916235 Test Loss: 0.4170847\n",
      "Test Accuracy 0.86328125\n",
      "Sample time 0.0771940016746521 Train time 0.11475135326385498\n",
      "*****Iter 3600*****\n",
      "Train Loss: 0.33414373 Test Loss: 0.37516022\n",
      "Test Accuracy 0.890625\n",
      "Sample time 0.06425928831100464 Train time 0.09573078632354737\n",
      "*****Iter 3700*****\n",
      "Train Loss: 0.35550565 Test Loss: 0.32631192\n",
      "Test Accuracy 0.9296875\n",
      "Sample time 0.06052823781967163 Train time 0.0891222333908081\n",
      "*****Iter 3800*****\n",
      "Train Loss: 0.34376433 Test Loss: 0.36678097\n",
      "Test Accuracy 0.91015625\n",
      "Sample time 0.06346813917160034 Train time 0.10295705318450928\n",
      "*****Iter 3900*****\n",
      "Train Loss: 0.3162689 Test Loss: 0.3710542\n",
      "Test Accuracy 0.8984375\n",
      "Sample time 0.07130921125411988 Train time 0.10773089647293091\n",
      "*****Iter 4000*****\n",
      "Train Loss: 0.2984382 Test Loss: 0.36874425\n",
      "Test Accuracy 0.90234375\n",
      "Sample time 0.06764798641204833 Train time 0.10709207057952881\n",
      "*****Iter 4100*****\n",
      "Train Loss: 0.3202566 Test Loss: 0.3682441\n",
      "Test Accuracy 0.90234375\n",
      "Sample time 0.07356815576553345 Train time 0.10933183431625366\n",
      "*****Iter 4200*****\n",
      "Train Loss: 0.33961183 Test Loss: 0.3645704\n",
      "Test Accuracy 0.890625\n",
      "Sample time 0.07706819295883179 Train time 0.11529755353927612\n",
      "*****Iter 4300*****\n",
      "Train Loss: 0.35369843 Test Loss: 0.3388062\n",
      "Test Accuracy 0.93359375\n",
      "Sample time 0.07700974702835083 Train time 0.11690580129623412\n",
      "*****Iter 4400*****\n",
      "Train Loss: 0.3579682 Test Loss: 0.34397247\n",
      "Test Accuracy 0.9375\n",
      "Sample time 0.06175156116485596 Train time 0.0936104154586792\n",
      "*****Iter 4500*****\n",
      "Train Loss: 0.35785094 Test Loss: 0.33423686\n",
      "Test Accuracy 0.9375\n",
      "Sample time 0.061669156551361085 Train time 0.09633901596069336\n",
      "*****Iter 4600*****\n",
      "Train Loss: 0.31382066 Test Loss: 0.32991725\n",
      "Test Accuracy 0.94921875\n",
      "Sample time 0.05816995620727539 Train time 0.08957001686096192\n",
      "*****Iter 4700*****\n",
      "Train Loss: 0.33258253 Test Loss: 0.36826554\n",
      "Test Accuracy 0.90234375\n",
      "Sample time 0.06000916004180908 Train time 0.09155085086822509\n",
      "*****Iter 4800*****\n",
      "Train Loss: 0.32233393 Test Loss: 0.33837456\n",
      "Test Accuracy 0.92578125\n",
      "Sample time 0.06116832733154297 Train time 0.09713958740234375\n",
      "*****Iter 4900*****\n",
      "Train Loss: 0.3104459 Test Loss: 0.34411404\n",
      "Test Accuracy 0.93359375\n",
      "Sample time 0.06791853904724121 Train time 0.10462143421173095\n",
      "*****Iter 5000*****\n",
      "Train Loss: 0.29251632 Test Loss: 0.3267712\n",
      "Test Accuracy 0.9453125\n",
      "Sample time 0.07377547025680542 Train time 0.11388142824172974\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_classes\", type=int, default=2)\n",
    "    parser.add_argument(\"--num_shot\", type=int, default=1)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--eval_freq\", type=int, default=100)\n",
    "    parser.add_argument(\"--meta_batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=256)\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=123)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--train_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--image_caching\", type=bool, default=True)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\")\n",
    "    parser.add_argument(\"--debug\", type=str, default=True)\n",
    "    parser.add_argument('--cache', action='store_true')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf599169-79a2-47c5-95d7-1ce29b5c311a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mann",
   "language": "python",
   "name": "mann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
