{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84ba1c5-281d-4295-9931-4b4bfc82da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "import imageio\n",
    "from torch.utils.data import dataset, sampler, dataloader\n",
    "\n",
    "\n",
    "NUM_INPUT_CHANNELS = 1\n",
    "NUM_HIDDEN_CHANNELS = 64\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 4\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "PRINT_INTERVAL = 10\n",
    "VAL_INTERVAL = PRINT_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 600\n",
    "\n",
    "NUM_TRAIN_CLASSES = 1100\n",
    "NUM_VAL_CLASSES = 100\n",
    "NUM_TEST_CLASSES = 423\n",
    "NUM_SAMPLES_PER_CLASS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3dc5ca-bffa-4eb9-a15e-2b78574e3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    \"\"\"Loads and transforms an Omniglot image\"\"\"\n",
    "    x = imageio.imread(file_path)\n",
    "    x = torch.tensor(x, dtype=torch.float32).reshape([1, 28, 28])\n",
    "    x = x / 255.0\n",
    "    return 1 - x\n",
    "\n",
    "\n",
    "class OmniglotDataset(dataset.Dataset):\n",
    "    \"\"\"Omniglot dataset for meta-learning.\n",
    "\n",
    "    Each element of the dataset is a task. A task is specified with a key,\n",
    "    which is a tuple of class indices (no particular order). The corresponding\n",
    "    value is the instantiated task, which consists of sampled (image, label)\n",
    "    pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    _BASE_PATH = './omniglot_resized'\n",
    "\n",
    "    def __init__(self, num_support, num_query):\n",
    "        \"\"\"Inits OmniglotDataset\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # get all character folders\n",
    "        self._character_folders = glob.glob(\n",
    "            os.path.join(self._BASE_PATH, '*/*/'))\n",
    "        assert len(self._character_folders) == (\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
    "        )\n",
    "\n",
    "        # shuffle characters\n",
    "        np.random.default_rng(0).shuffle(self._character_folders)\n",
    "\n",
    "        # check problem arguments\n",
    "        assert num_support + num_query <= NUM_SAMPLES_PER_CLASS\n",
    "        self._num_support = num_support\n",
    "        self._num_query = num_query\n",
    "\n",
    "    def __getitem__(self, class_idxs):\n",
    "        \"\"\"Constructs a task.\n",
    "\n",
    "        Data for each class is sampled uniformly at random without replacement.\n",
    "        \"\"\"\n",
    "        images_support, images_query = [], []\n",
    "        labels_support, labels_query = [], []\n",
    "\n",
    "        for label, class_idx in enumerate(class_idxs):\n",
    "            # get a class's examples and sample from them\n",
    "            all_file_paths = glob.glob(\n",
    "                os.path.join(self._character_folders[class_idx], '*.png')\n",
    "            )\n",
    "            sampled_file_paths = np.random.default_rng().choice(\n",
    "                all_file_paths,\n",
    "                size=self._num_support + self._num_query,\n",
    "                replace=False\n",
    "            )\n",
    "            images = [load_image(file_path) for file_path in sampled_file_paths]\n",
    "\n",
    "            # split sampled examples into support and query\n",
    "            images_support.extend(images[:self._num_support])\n",
    "            images_query.extend(images[self._num_support:])\n",
    "            labels_support.extend([label] * self._num_support)\n",
    "            labels_query.extend([label] * self._num_query)\n",
    "\n",
    "        # aggregate into tensors\n",
    "        images_support = torch.stack(images_support)  # shape (N*S, C, H, W)\n",
    "        labels_support = torch.tensor(labels_support)  # shape (N*S)\n",
    "        images_query = torch.stack(images_query)\n",
    "        labels_query = torch.tensor(labels_query)\n",
    "\n",
    "        return images_support, labels_support, images_query, labels_query\n",
    "\n",
    "\n",
    "class OmniglotSampler(sampler.Sampler):\n",
    "    \"\"\"Samples task specification keys for an OmniglotDataset.\"\"\"\n",
    "    def __init__(self, split_idxs, num_way, num_tasks):\n",
    "        \"\"\"Inits OmniglotSampler\"\"\"\n",
    "        super().__init__(None)\n",
    "        self._split_idxs = split_idxs\n",
    "        self._num_way = num_way\n",
    "        self._num_tasks = num_tasks\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (\n",
    "            np.random.default_rng().choice(\n",
    "                self._split_idxs,\n",
    "                size=self._num_way,\n",
    "                replace=False\n",
    "            ) for _ in range(self._num_tasks)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._num_tasks\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def get_omniglot_dataloader(\n",
    "        split,\n",
    "        batch_size,\n",
    "        num_way,\n",
    "        num_support,\n",
    "        num_query,\n",
    "        num_tasks_per_epoch,\n",
    "        num_workers=0,\n",
    "):\n",
    "    \"\"\"Returns a dataloader.DataLoader for Omniglot\"\"\"\n",
    "\n",
    "    if split == 'train':\n",
    "        split_idxs = range(NUM_TRAIN_CLASSES)\n",
    "    elif split == 'val':\n",
    "        split_idxs = range(\n",
    "            NUM_TRAIN_CLASSES,\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES\n",
    "        )\n",
    "    elif split == 'test':\n",
    "        split_idxs = range(\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES,\n",
    "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return dataloader.DataLoader(\n",
    "        dataset=OmniglotDataset(num_support, num_query),\n",
    "        batch_size=batch_size,\n",
    "        sampler=OmniglotSampler(split_idxs, num_way, num_tasks_per_epoch),\n",
    "        num_workers=0,\n",
    "        collate_fn=identity,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebcc11a-b2f4-4345-a003-34a19fadfaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(logits, labels):\n",
    "    \"\"\"Returns the mean accuracy of a model's predictions on a set of examples\"\"\"\n",
    "    y = torch.argmax(logits, dim=-1) == labels\n",
    "    y = y.type(torch.float)\n",
    "    return torch.mean(y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a53b677-a08b-40aa-86b4-8dfaf544af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNetNetwork(nn.Module):\n",
    "    \"\"\"Container for ProtoNet weights and image-to-latent computation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inits ProtoNetNetwork.\n",
    "\n",
    "        The network consists of four convolutional blocks, each comprising a\n",
    "        convolution layer, a batch normalization layer, ReLU activation, and 2x2\n",
    "        max pooling for downsampling. There is an additional flattening\n",
    "        operation at the end.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for _ in range(NUM_CONV_LAYERS):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    (KERNEL_SIZE, KERNEL_SIZE),\n",
    "                    padding='same'\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm2d(NUM_HIDDEN_CHANNELS))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "        layers.append(nn.Flatten())\n",
    "        self._layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Computes the latent representation of a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing a batch of latent representations\n",
    "                shape (num_images, latents)\n",
    "        \"\"\"\n",
    "        return self._layers(images)\n",
    "\n",
    "\n",
    "class ProtoNet:\n",
    "    \"\"\"Trains and assesses a prototypical network.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self._network = ProtoNetNetwork()\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._network.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _step(self, task_batch):\n",
    "        \"\"\"Computes ProtoNet mean loss (and accuracy) on a batch of tasks.\"\"\"\n",
    "        loss_batch = []\n",
    "        accuracy_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for task in task_batch:\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "\n",
    "            support_emb = self._network(images_support)\n",
    "            query_emb = self._network(images_query)\n",
    "\n",
    "            # Calculating the prototypes as the mean position of\n",
    "            # all the support images from3325 a given class\n",
    "            prototypes = []\n",
    "            for label in labels_support.unique():\n",
    "                prototype = support_emb[labels_support == label].mean(dim=0)\n",
    "                prototypes.append(prototype)\n",
    "            prototypes = torch.stack(prototypes)\n",
    "\n",
    "            query_emb = torch.stack([query_emb] * labels_support.unique().shape[0], dim=1)\n",
    "            query_prototypes = torch.stack([prototypes] * query_emb.shape[0])\n",
    "            query_preds = torch.norm(query_prototypes - query_emb, dim=-1).square()\n",
    "\n",
    "            support_emb = torch.stack([support_emb] * labels_support.unique().shape[0], dim=1)\n",
    "            support_prototypes = torch.stack([prototypes] * support_emb.shape[0])\n",
    "            support_preds = torch.norm(support_prototypes - support_emb, dim=-1).square()\n",
    "\n",
    "            loss = F.cross_entropy(-query_preds, labels_query)\n",
    "            support_acc = score(-support_preds, labels_support)\n",
    "            query_acc = score(-query_preds, labels_query)\n",
    "\n",
    "            loss_batch.append(loss)\n",
    "            accuracy_support_batch.append(support_acc)\n",
    "            accuracy_query_batch.append(query_acc)\n",
    "        return (\n",
    "            torch.mean(torch.stack(loss_batch)),\n",
    "            np.mean(accuracy_support_batch),\n",
    "            np.mean(accuracy_query_batch)\n",
    "        )\n",
    "\n",
    "    def train(self, dataloader_train, dataloader_val):\n",
    "        \"\"\"Train the ProtoNet.\n",
    "\n",
    "        Consumes dataloader_train to optimize weights of ProtoNetNetwork\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        for i_step, task_batch in enumerate(\n",
    "                dataloader_train,\n",
    "                start=self._start_train_step\n",
    "        ):\n",
    "            self._optimizer.zero_grad()\n",
    "            loss, accuracy_support, accuracy_query = self._step(task_batch)\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % PRINT_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {loss.item():.3f}, '\n",
    "                    f'support accuracy: {accuracy_support.item():.3f}, '\n",
    "                    f'query accuracy: {accuracy_query.item():.3f}'\n",
    "                )\n",
    "\n",
    "            if i_step % VAL_INTERVAL == 0:\n",
    "                with torch.no_grad():\n",
    "                    losses, accuracies_support, accuracies_query = [], [], []\n",
    "                    for val_task_batch in dataloader_val:\n",
    "                        loss, accuracy_support, accuracy_query = (\n",
    "                            self._step(val_task_batch)\n",
    "                        )\n",
    "                        losses.append(loss.item())\n",
    "                        accuracies_support.append(accuracy_support)\n",
    "                        accuracies_query.append(accuracy_query)\n",
    "                    loss = np.mean(losses)\n",
    "                    accuracy_support = np.mean(accuracies_support)\n",
    "                    accuracy_query = np.mean(accuracies_query)\n",
    "                print(\n",
    "                    f'Validation: '\n",
    "                    f'loss: {loss:.3f}, '\n",
    "                    f'support accuracy: {accuracy_support:.3f}, '\n",
    "                    f'query accuracy: {accuracy_query:.3f}'\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe82c7e6-f6e4-4704-b429-52f7279ed7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    protonet = ProtoNet(args.learning_rate)\n",
    "\n",
    "    if args.checkpoint_step > -1:\n",
    "        protonet.load(args.checkpoint_step)\n",
    "    else:\n",
    "        print('Checkpoint loading skipped.')\n",
    "\n",
    "    if not args.test:\n",
    "        num_training_tasks = args.batch_size * (args.num_train_iterations -\n",
    "                                                args.checkpoint_step - 1)\n",
    "        print(\n",
    "            f'Training on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_train = get_omniglot_dataloader(\n",
    "            'train',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            num_training_tasks\n",
    "        )\n",
    "        dataloader_val = get_omniglot_dataloader(\n",
    "            'val',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            args.batch_size * 4\n",
    "        )\n",
    "        protonet.train(\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3567a6-a905-48f4-98d7-4708b60fe41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(log_dir=None, num_way=5, num_support=1, num_query=15, learning_rate=0.001, batch_size=16, num_train_iterations=50, test=False, checkpoint_step=-1, cache=False, device='cpu')\n",
      "Checkpoint loading skipped.\n",
      "Training on tasks with composition num_way=5, num_support=1, num_query=15\n",
      "Starting training at iteration 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_4364\\831245959.py:3: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  x = imageio.imread(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss: 3.618, support accuracy: 1.000, query accuracy: 0.529\n",
      "Validation: loss: 1.842, support accuracy: 1.000, query accuracy: 0.656\n",
      "Iteration 10: loss: 0.672, support accuracy: 1.000, query accuracy: 0.764\n",
      "Iteration 20: loss: 0.449, support accuracy: 1.000, query accuracy: 0.843\n",
      "Iteration 30: loss: 0.311, support accuracy: 1.000, query accuracy: 0.889\n",
      "Iteration 40: loss: 0.326, support accuracy: 1.000, query accuracy: 0.887\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('Train a ProtoNet!')\n",
    "    parser.add_argument('--log_dir', type=str, default=None,\n",
    "                        help='directory to save to or load from')\n",
    "    parser.add_argument('--num_way', type=int, default=5,\n",
    "                        help='number of classes in a task')\n",
    "    parser.add_argument('--num_support', type=int, default=1,\n",
    "                        help='number of support examples per class in a task')\n",
    "    parser.add_argument('--num_query', type=int, default=15,\n",
    "                        help='number of query examples per class in a task')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                        help='learning rate for the network')\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='number of tasks per outer-loop update')\n",
    "    parser.add_argument('--num_train_iterations', type=int, default=50,\n",
    "                        help='number of outer-loop updates to train for')\n",
    "    parser.add_argument('--test', default=False, action='store_true',\n",
    "                        help='train or test')\n",
    "    parser.add_argument('--checkpoint_step', type=int, default=-1,\n",
    "                        help=('checkpoint iteration to load for resuming '\n",
    "                              'training, or for evaluation (-1 is ignored)'))\n",
    "    parser.add_argument('--cache', action='store_true')\n",
    "    parser.add_argument('--device', type=str, default='cpu')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f5137-c216-4a50-8309-5dcf353b635f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mann",
   "language": "python",
   "name": "mann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
